{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = r\"C:\\Users\\Sistem Temu Kembali Informasi\\Tugas dan Latihan\\Final tugas akhir\\IR\\UU + Perpu.csv\"\n",
    "stopword_id = r'C:\\Users\\Sistem Temu Kembali Informasi\\Tugas dan Latihan\\Final tugas akhir\\IR\\stopwords-id.txt'\n",
    "\n",
    "pdf_folder_Perpu = r\"C:\\Users\\Sistem Temu Kembali Informasi\\Tugas dan Latihan\\Final tugas akhir\\PERPU\\File Perpu\"\n",
    "pdf_folder_UU = r\"C:\\Users\\Sistem Temu Kembali Informasi\\Tugas dan Latihan\\Final tugas akhir\\UU\\File UU\"\n",
    "parent_folder_hasil = r\"C:\\Users\\Sistem Temu Kembali Informasi\\Tugas dan Latihan\\Final tugas akhir\\Relevance Feedback\\Hasil\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: korupsi\n",
      "\n",
      "\n",
      "Menggunakan feedback relevansi yang telah disimpan...\n",
      "\n",
      "\n",
      "\n",
      " -- HASIL PENELUSURAN ULANG -- \n",
      "\n",
      "\n",
      "Dokumen 1:\n",
      "Judul: Undang-Undang Nomor 30 Tahun 2002\n",
      "Similarity: 0.7687\n",
      "\n",
      "Dokumen 2:\n",
      "Judul: Undang-Undang Nomor 19 Tahun 2019\n",
      "Similarity: 0.7044\n",
      "\n",
      "Dokumen 3:\n",
      "Judul: Peraturan Pemerintah Pengganti Undang-Undang Nomor 1 Tahun 2015\n",
      "Similarity: 0.6410\n",
      "\n",
      "Dokumen 4:\n",
      "Judul: Undang-Undang Nomor 10 Tahun 2015\n",
      "Similarity: 0.6297\n",
      "\n",
      "Dokumen 5:\n",
      "Judul: Undang-Undang Nomor 31 Tahun 1999\n",
      "Similarity: 0.6082\n",
      "\n",
      "Dokumen 6:\n",
      "Judul: Undang-Undang Nomor 20 Tahun 2001\n",
      "Similarity: 0.5766\n",
      "\n",
      "Dokumen 7:\n",
      "Judul: Undang-Undang Nomor 46 Tahun 2009\n",
      "Similarity: 0.5546\n",
      "\n",
      "Dokumen 8:\n",
      "Judul: Undang-Undang Nomor 3 Tahun 1971\n",
      "Similarity: 0.5147\n",
      "\n",
      "Dokumen 9:\n",
      "Judul: Undang-Undang Nomor 7 Tahun 2006\n",
      "Similarity: 0.4243\n",
      "\n",
      "Dokumen 10:\n",
      "Judul: Undang-Undang Nomor 1 Tahun 2023\n",
      "Similarity: 0.2634\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import pickle\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from joblib import Parallel, delayed\n",
    "from joblib import Memory\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Caching setup\n",
    "location = './cacheall'\n",
    "memory = Memory(location, verbose=0)\n",
    "\n",
    "# Inisialisasi stemmer bahasa Indonesia\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "# Membaca daftar stop words bahasa Indonesia\n",
    "# stopword_id = 'stopwords.txt'  # Specify the path to your stopwords file\n",
    "with open(stopword_id, 'r') as f:\n",
    "    stop_words_id = f.read().splitlines()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Pastikan text adalah string\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    \n",
    "    # Menghilangkan karakter berulang\n",
    "    text = re.sub(r'(.)\\1+', r'\\1', text)\n",
    "    \n",
    "    # Menghilangkan angka\n",
    "    text = ''.join([i for i in text if not i.isdigit()])\n",
    "    \n",
    "    # Menghilangkan tanda baca\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Mengubah teks menjadi huruf kecil\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Melakukan stemming pada teks\n",
    "    text = stemmer.stem(text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "@memory.cache\n",
    "def preprocess_parallel(text_series):\n",
    "    return Parallel(n_jobs=-1)(delayed(stemmer.stem)(text) for text in text_series)\n",
    "\n",
    "# Membaca file CSV\n",
    "# data = 'data.csv'  # Specify the path to your CSV file\n",
    "df = pd.read_csv(data)\n",
    "\n",
    "# Pastikan tidak ada nilai NaN dan semua nilai adalah string\n",
    "df['Teks'] = preprocess_parallel(df['Teks'].fillna(''))\n",
    "\n",
    "# Inisialisasi TfidfVectorizer dengan stop words bahasa Indonesia\n",
    "tfidf = TfidfVectorizer(stop_words=stop_words_id, max_df=0.85, min_df=2, ngram_range=(1, 2))\n",
    "\n",
    "# Melakukan fit dan transformasi pada kolom Teks\n",
    "tfidf_matrix = tfidf.fit_transform(df['Teks'])\n",
    "\n",
    "# Menggunakan TruncatedSVD untuk pengurangan dimensi\n",
    "svd = TruncatedSVD(n_components=100)\n",
    "tfidf_matrix_reduced = svd.fit_transform(tfidf_matrix)\n",
    "\n",
    "# Path untuk menyimpan feedback relevansi\n",
    "feedback_path = 'relevance_feedback.pkl'\n",
    "\n",
    "# Memuat feedback relevansi jika file ada, jika tidak, buat dictionary kosong\n",
    "try:\n",
    "    with open(feedback_path, 'rb') as f:\n",
    "        relevance_feedback = pickle.load(f)\n",
    "except FileNotFoundError:\n",
    "    relevance_feedback = {}\n",
    "\n",
    "def search_documents(query, top_n=10):\n",
    "    # Preprocessing query\n",
    "    query = preprocess_text(query)\n",
    "    \n",
    "    # Transformasi query menjadi vektor tf-idf\n",
    "    query_vec = tfidf.transform([query])\n",
    "    query_vec_reduced = svd.transform(query_vec)\n",
    "    \n",
    "    # Menghitung cosine similarity antara query dan semua dokumen\n",
    "    cosine_similarities = linear_kernel(query_vec_reduced, tfidf_matrix_reduced).flatten()\n",
    "    \n",
    "    # Mendapatkan indeks dokumen dengan similarity tertinggi\n",
    "    related_docs_indices = cosine_similarities.argsort()[-top_n:][::-1]\n",
    "    \n",
    "    # Mendapatkan judul, teks, dan nilai similarity dari dokumen yang relevan\n",
    "    results = [(df.iloc[i]['Judul'], df.iloc[i]['Teks'], cosine_similarities[i], i) for i in related_docs_indices]\n",
    "    \n",
    "    return results\n",
    "\n",
    "def get_feedback(results):\n",
    "    feedback = []\n",
    "    for idx, (title, text, similarity, doc_index) in enumerate(results):\n",
    "        print(f\"Dokumen {idx + 1}:\")\n",
    "        print(f\"Judul: {title}\")\n",
    "        print(f\"Teks: {text[:200]}...\")  # Display only the first 200 characters\n",
    "        print(f\"Similarity: {similarity:.4f}\")\n",
    "        relevansi = int(input(\"Masukkan nilai relevansi (1-5): \"))\n",
    "        feedback.append((doc_index, relevansi))\n",
    "    return feedback\n",
    "\n",
    "def optimize_with_feedback(feedback, tfidf_matrix_reduced):\n",
    "    relevant_docs = [idx for idx, relevansi in feedback if relevansi >= 3]\n",
    "    non_relevant_docs = [idx for idx, relevansi in feedback if relevansi < 3]\n",
    "    \n",
    "    if not relevant_docs:\n",
    "        print(\"Tidak ada dokumen yang dianggap relevan. Pencarian ulang tidak dapat dilakukan.\")\n",
    "        return None\n",
    "    \n",
    "    relevant_matrix = tfidf_matrix_reduced[relevant_docs]\n",
    "    non_relevant_matrix = tfidf_matrix_reduced[non_relevant_docs] if non_relevant_docs else np.zeros(relevant_matrix.shape)\n",
    "    \n",
    "    # Compute the centroid of relevant and non-relevant documents\n",
    "    relevant_centroid = np.asarray(relevant_matrix.mean(axis=0)).flatten()\n",
    "    non_relevant_centroid = np.asarray(non_relevant_matrix.mean(axis=0)).flatten() if non_relevant_docs else np.zeros(relevant_centroid.shape)\n",
    "    \n",
    "    # Update query vector by moving it towards the relevant centroid and away from the non-relevant centroid\n",
    "    def adjust_query_vec(query_vec, relevant_centroid, non_relevant_centroid, alpha=1, beta=0.75, gamma=0.15):\n",
    "        return alpha * query_vec + beta * relevant_centroid - gamma * non_relevant_centroid\n",
    "    \n",
    "    return adjust_query_vec\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    query = input(\"Masukkan Query: \")\n",
    "    print(f\"Query: {query}\\n\")\n",
    "    print()\n",
    "    \n",
    "    if query in relevance_feedback:\n",
    "        print(\"Menggunakan feedback relevansi yang telah disimpan...\\n\")\n",
    "        feedback = relevance_feedback[query]\n",
    "        adjust_query_vec = optimize_with_feedback(feedback, tfidf_matrix_reduced)\n",
    "        \n",
    "        if adjust_query_vec:\n",
    "            relevant_docs = [idx for idx, relevansi in feedback if relevansi >= 3]\n",
    "            non_relevant_docs = [idx for idx, relevansi in feedback if relevansi < 3]\n",
    "            \n",
    "            # Reprocess the query with the adjusted query vector\n",
    "            query_vec = tfidf.transform([preprocess_text(query)])\n",
    "            query_vec_reduced = svd.transform(query_vec)\n",
    "            adjusted_query_vec = adjust_query_vec(query_vec_reduced, np.asarray(tfidf_matrix_reduced[relevant_docs].mean(axis=0)).flatten(), \n",
    "                                                  np.asarray(tfidf_matrix_reduced[non_relevant_docs].mean(axis=0)).flatten() if non_relevant_docs else np.zeros(query_vec_reduced.shape))\n",
    "            \n",
    "            # Compute cosine similarity with the adjusted query vector\n",
    "            cosine_similarities = linear_kernel(adjusted_query_vec, tfidf_matrix_reduced).flatten()\n",
    "            related_docs_indices = cosine_similarities.argsort()[-10:][::-1]\n",
    "            \n",
    "            # Display optimized results\n",
    "            optimized_results = [(df.iloc[i]['Judul'], df.iloc[i]['Teks'], cosine_similarities[i]) for i in related_docs_indices]\n",
    "            print(\"\\n\\n -- HASIL PENELUSURAN ULANG -- \\n\\n\")\n",
    "            for idx, (title, text, similarity) in enumerate(optimized_results):\n",
    "                print(f\"Dokumen {idx + 1}:\")\n",
    "                print(f\"Judul: {title}\")\n",
    "                print(f\"Similarity: {similarity:.4f}\")\n",
    "                print()\n",
    "    else:\n",
    "        initial_results = search_documents(query)\n",
    "        feedback = get_feedback(initial_results)\n",
    "        relevance_feedback[query] = feedback  # Simpan feedback untuk query ini\n",
    "        with open(feedback_path, 'wb') as f:\n",
    "            pickle.dump(relevance_feedback, f)  # Simpan feedback relevansi ke file\n",
    "        adjust_query_vec = optimize_with_feedback(feedback, tfidf_matrix_reduced)\n",
    "        \n",
    "        if adjust_query_vec:\n",
    "            relevant_docs = [idx for idx, relevansi in feedback if relevansi >= 3]\n",
    "            non_relevant_docs = [idx for idx, relevansi in feedback if relevansi < 3]\n",
    "            \n",
    "            # Reprocess the query with the adjusted query vector\n",
    "            query_vec = tfidf.transform([preprocess_text(query)])\n",
    "            query_vec_reduced = svd.transform(query_vec)\n",
    "            adjusted_query_vec = adjust_query_vec(query_vec_reduced, np.asarray(tfidf_matrix_reduced[relevant_docs].mean(axis=0)).flatten(), \n",
    "                                                  np.asarray(tfidf_matrix_reduced[non_relevant_docs].mean(axis=0)).flatten() if non_relevant_docs else np.zeros(query_vec_reduced.shape))\n",
    "            \n",
    "            # Compute cosine similarity with the adjusted query vector\n",
    "            cosine_similarities = linear_kernel(adjusted_query_vec, tfidf_matrix_reduced).flatten()\n",
    "            related_docs_indices = cosine_similarities.argsort()[-10:][::-1]\n",
    "            \n",
    "            # Display optimized results\n",
    "            optimized_results = [(df.iloc[i]['Judul'], df.iloc[i]['Teks'], cosine_similarities[i]) for i in related_docs_indices]\n",
    "            print(\"\\n\\n -- HASIL PENELUSURAN ULANG -- \\n\\n\")\n",
    "            for idx, (title, text, similarity) in enumerate(optimized_results):\n",
    "                print(f\"Dokumen {idx + 1}:\")\n",
    "                print(f\"Judul: {title}\")\n",
    "                print(f\"Similarity: {similarity:.4f}\")\n",
    "                print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dokumen disimpan di folder C:\\Users\\muham\\OneDrive - Universitas Airlangga\\Semester 6\\Sistem Temu Kembali Informasi\\Tugas dan Latihan\\Final tugas akhir\\Relevance Feedback\\Hasil\\korupsi\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# Membuat folder parent jika belum ada\n",
    "os.makedirs(parent_folder_hasil, exist_ok=True)\n",
    "\n",
    "for idx, (title, text, similarity) in enumerate(optimized_results):\n",
    "    found = False\n",
    "    # Mencari file PDF dengan judul yang sesuai di folder1\n",
    "    pdf_file = os.path.join(pdf_folder_Perpu, f\"{title}.pdf\")\n",
    "    if os.path.isfile(pdf_file):\n",
    "        found = True\n",
    "    else:\n",
    "        # Mencari file PDF dengan judul yang sesuai di folder2\n",
    "        pdf_file = os.path.join(pdf_folder_UU, f\"{title}.pdf\")\n",
    "        if os.path.isfile(pdf_file):\n",
    "            found = True\n",
    "\n",
    "    if found:\n",
    "        # Buat folder output sesuai dengan query\n",
    "        output_folder = os.path.join(parent_folder_hasil, query)\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "        # Menyalin file PDF ke folder output\n",
    "        output_file = os.path.join(output_folder, f\"{title}.pdf\")\n",
    "        shutil.copy(pdf_file, output_file)\n",
    "    else:\n",
    "        print(f\"File PDF untuk {title} tidak ditemukan.\")\n",
    "\n",
    "print(f\"Dokumen disimpan di folder {output_folder}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tambahan kode untuk menghitung presisi\n",
    "# # Judul dokumen hasil awal\n",
    "# judul_hasil_awal = [title for title, _, _ in initial_results]\n",
    "\n",
    "# # Judul dokumen hasil setelah relevance feedback\n",
    "# judul_hasil_feedback = [title for title, _, _ in optimized_results]\n",
    "\n",
    "# # Menghitung jumlah dokumen yang sama dalam hasil setelah feedback\n",
    "# jumlah_dokumen_sama = sum([1 for doc in judul_hasil_feedback if doc in judul_hasil_awal])\n",
    "\n",
    "# # Jumlah dokumen yang diambil setelah feedback\n",
    "# total_dokumen_diambil = len(judul_hasil_feedback)\n",
    "\n",
    "# # Menghitung presisi\n",
    "# presisi = jumlah_dokumen_sama / total_dokumen_diambil\n",
    "\n",
    "# print(f'Presisi: {presisi:.2f}')  # Output: Presisi\n",
    "\n",
    "# # Menghitung presisi awal\n",
    "# jumlah_dokumen_sama_awal = sum([1 for doc in judul_hasil_awal if doc in judul_hasil_awal])\n",
    "# total_dokumen_diambil_awal = len(judul_hasil_awal)\n",
    "# presisi_awal = jumlah_dokumen_sama_awal / total_dokumen_diambil_awal\n",
    "\n",
    "# print(f'Presisi Awal: {presisi_awal:.2f}')  # Output: Presisi Awal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Definisikan bobot untuk setiap nilai relevansi\n",
    "# relevance_weights = {1: -2, 2: -1, 3: 0, 4: 1, 5: 2}\n",
    "\n",
    "# def adjust_query_vec(query_vec, relevant_docs, non_relevant_docs):\n",
    "#     alpha = 0.75  # Bobot untuk dokumen relevan\n",
    "#     beta = 0.25  # Bobot untuk dokumen tidak relevan\n",
    "\n",
    "#     # Menghitung mean vector dengan bobot relevansi\n",
    "#     relevant_mean_vec = np.average(\n",
    "#         [tfidf_matrix_reduced[idx] for idx in relevant_docs], \n",
    "#         axis=0, \n",
    "#         weights=[relevance_weights[relevance_feedback[idx]] for idx in relevant_docs]\n",
    "#     )\n",
    "\n",
    "#     non_relevant_mean_vec = np.average(\n",
    "#         [tfidf_matrix_reduced[idx] for idx in non_relevant_docs], \n",
    "#         axis=0, \n",
    "#         weights=[relevance_weights[relevance_feedback[idx]] for idx in non_relevant_docs]\n",
    "#     ) if non_relevant_docs else np.zeros(query_vec.shape)\n",
    "\n",
    "#     adjusted_query_vec = query_vec + alpha * relevant_mean_vec - beta * non_relevant_mean_vec\n",
    "#     return adjusted_query_vec\n",
    "\n",
    "# # Mengumpulkan dokumen relevan dan tidak relevan berdasarkan feedback\n",
    "# relevant_docs = [idx for idx, relevansi in feedback if relevansi >= 3]\n",
    "# non_relevant_docs = [idx for idx, relevansi in feedback if relevansi < 3]\n",
    "\n",
    "# # Optimisasi query dengan feedback\n",
    "# query_vec = tfidf.transform([preprocess_text(query)])\n",
    "# query_vec_reduced = svd.transform(query_vec)\n",
    "# adjusted_query_vec = adjust_query_vec(query_vec_reduced, relevant_docs, non_relevant_docs)\n",
    "\n",
    "# # Menghitung kesamaan kosinus dengan vektor query yang sudah dioptimalkan\n",
    "# cosine_similarities = linear_kernel(adjusted_query_vec, tfidf_matrix_reduced).flatten()\n",
    "# related_docs_indices = cosine_similarities.argsort()[-10:][::-1]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
