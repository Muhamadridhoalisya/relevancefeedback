{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading preprocessed data from cache...\n",
      "Query: tanah\n",
      "\n",
      "\n",
      "Dokumen 1:\n",
      "Judul: Peraturan Pemerintah Pengganti Undang-Undang Nomor 2 Tahun 2007\n",
      "Teks: atur perintah anti undangundang republik indonesia nomor tahun tentang tangan masalah hukum dalam rangka pelaksanan rehabilitasi dan rekonstruksi wilayah dan hidup masyarakat di provinsi nangroe aceh ...\n",
      "Similarity: 0.4647\n",
      "Input tidak valid. Masukkan 1/ya atau 0/tidak.\n",
      "Dokumen 2:\n",
      "Judul: Peraturan Pemerintah Pengganti Undang-Undang Nomor 1 Tahun 1997\n",
      "Teks: atur perintah anti undang undang republik indonesia nomor tahun tentang penanguhan mulai laku undang undang nomor tahun tentang bea oleh hak atas tanah dan bangun presiden republik indonesia timbang a...\n",
      "Similarity: 0.2356\n",
      "Dokumen 3:\n",
      "Judul: Peraturan Pemerintah Pengganti Undang-Undang Nomor 1 Tahun 2008\n",
      "Teks: lembar negararepublik indonesiano otonomi khusus pemerintahanpemerintah daerah papua penjelasandalam tambah lembar negara republikindonesia nomor atur pemerintahpenganti undangundang republik indonesi...\n",
      "Similarity: 0.0107\n",
      "Dokumen 4:\n",
      "Judul: Peraturan Pemerintah Pengganti Undang-undang Nomor 2 Tahun 2022\n",
      "Teks: menimbangpresidenrepublik indonesiaperaturan pemerintahpenganti undangundang republik indonesianomor tahvn tentangcipta kerjadengan rahmat tuhan yang maha esapresiden republik indonesiaa bahwa untuk w...\n",
      "Similarity: 0.0049\n",
      "Dokumen 5:\n",
      "Judul: Peraturan Pemerintah Pengganti Undang-Undang Nomor 1 Tahun 2002\n",
      "Teks: presidenrepublik indonesiaperaturan pemerintahpenganti undang undang republik indonesianomor tahun tentangpemberantasan tindak pidana terorismepresiden republik indonesiamenimbang abahwa dalam wujud t...\n",
      "Similarity: 0.0049\n",
      "Dokumen 6:\n",
      "Judul: Peraturan Pemerintah Pengganti Undang-Undang Nomor 2 Tahun 2005\n",
      "Teks: presiden republik indonesia atur perintah anti undangundang republik indonesia nomor tahun tentang badan rehabilitasi dan rekonstruksi wi layah dan hidup masyarakat provinsi nangr oe aceh darusalam da...\n",
      "Similarity: 0.0044\n",
      "Dokumen 7:\n",
      "Judul: Peraturan Pemerintah Pengganti Undang-Undang Nomor 1 Tahun 2022\n",
      "Teks: salinanpresidenrepublik indonesiaperaturan pemerintahpenganti undangundang republik indonesianomor l tahun tentangperubahan atas undangundang nomor tahun tentangpemilihan umumdengan rahmat tuhan yang ...\n",
      "Similarity: 0.0040\n",
      "Dokumen 8:\n",
      "Judul: Peraturan Pemerintah Pengganti Undang-Undang Nomor 2 Tahun 2014\n",
      "Teks: tambah lembar negara ri no perintah daerah otonomi pilih kepala daerah ubah jelas atas lembar negara republik indonesia tahun nomor jelas atas atur perintah anti undangundang republik indonesia nomor ...\n",
      "Similarity: 0.0000\n",
      "Dokumen 9:\n",
      "Judul: Peraturan Pemerintah Pengganti Undang-Undang Nomor 2 Tahun 2020\n",
      "Teks: lembar negara republik indonesia no perintah daerah pilih kepala daerah ubah jelas dalam tambah lembar negara republik indonesia nomor atur perintah anti undang undang republik indonesia nomor tahun t...\n",
      "Similarity: 0.0000\n",
      "Dokumen 10:\n",
      "Judul: Peraturan Pemerintah Pengganti Undang-Undang Nomor 1 Tahun 2006\n",
      "Teks: atur pemerintahpenganti undangundang republik indonesianomor tahun tentangperubahan dua atas undangundang nomor tahun tentang pilih umum angota dewan wakil rakyatdewan wakil daerah dan dewan wakil rak...\n",
      "Similarity: 0.0000\n",
      "\n",
      "\n",
      " -- HASIL PENELUSURAN ULANG -- \n",
      "\n",
      "\n",
      "Dokumen 1:\n",
      "Judul: Peraturan Pemerintah Pengganti Undang-Undang Nomor 2 Tahun 2007\n",
      "Similarity: 0.7812\n",
      "\n",
      "Dokumen 2:\n",
      "Judul: Peraturan Pemerintah Pengganti Undang-Undang Nomor 1 Tahun 1997\n",
      "Similarity: 0.5301\n",
      "\n",
      "Dokumen 3:\n",
      "Judul: Peraturan Pemerintah Pengganti Undang-Undang Nomor 1 Tahun 2008\n",
      "Similarity: 0.2812\n",
      "\n",
      "Dokumen 4:\n",
      "Judul: Peraturan Pemerintah Pengganti Undang-Undang Nomor 1 Tahun 2022\n",
      "Similarity: 0.1713\n",
      "\n",
      "Dokumen 5:\n",
      "Judul: Peraturan Pemerintah Pengganti Undang-Undang Nomor 2 Tahun 2005\n",
      "Similarity: 0.1347\n",
      "\n",
      "Dokumen 6:\n",
      "Judul: Peraturan Pemerintah Pengganti Undang-Undang Nomor 1 Tahun 2005\n",
      "Similarity: 0.0977\n",
      "\n",
      "Dokumen 7:\n",
      "Judul: Peraturan Pemerintah Pengganti Undang-Undang Nomor 1 Tahun 1984\n",
      "Similarity: 0.0887\n",
      "\n",
      "Dokumen 8:\n",
      "Judul: Peraturan Pemerintah Pengganti Undang-Undang Nomor 3 Tahun 1998\n",
      "Similarity: 0.0881\n",
      "\n",
      "Dokumen 9:\n",
      "Judul: Peraturan Pemerintah Pengganti Undang-Undang Nomor 1 Tahun 2004\n",
      "Similarity: 0.0878\n",
      "\n",
      "Dokumen 10:\n",
      "Judul: Peraturan Pemerintah Pengganti Undang-Undang Nomor 1 Tahun 2020\n",
      "Similarity: 0.0854\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = r\"/home/whoami/Programming/information retrieval/Relevance Feedback/UU + Perpu.csv\"\n",
    "stopword_id = r\"/home/whoami/Programming/information retrieval/Relevance Feedback/stopwords-id.txt\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import pickle\n",
    "import os\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from joblib import Parallel, delayed\n",
    "from joblib import Memory, dump, load\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Caching setup - folder untuk menyimpan cache\n",
    "cache_dir = './cacheall'\n",
    "memory = Memory(cache_dir, verbose=0)\n",
    "\n",
    "# Folder untuk menyimpan hasil preprocessing\n",
    "preprocessed_dir = './preprocessed_data'\n",
    "os.makedirs(preprocessed_dir, exist_ok=True)\n",
    "\n",
    "# Path untuk file yang akan menyimpan hasil preprocessing\n",
    "preprocessed_file = os.path.join(preprocessed_dir, 'preprocessed_texts.pkl')\n",
    "tfidf_matrix_file = os.path.join(preprocessed_dir, 'tfidf_matrix.pkl')\n",
    "tfidf_vectorizer_file = os.path.join(preprocessed_dir, 'tfidf_vectorizer.pkl')\n",
    "svd_model_file = os.path.join(preprocessed_dir, 'svd_model.pkl')\n",
    "tfidf_matrix_reduced_file = os.path.join(preprocessed_dir, 'tfidf_matrix_reduced.pkl')\n",
    "df_file = os.path.join(preprocessed_dir, 'dataframe.pkl')\n",
    "\n",
    "# Inisialisasi stemmer bahasa Indonesia\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "# Membaca daftar stop words bahasa Indonesia\n",
    "with open(stopword_id, 'r') as f:\n",
    "    stop_words_id = f.read().splitlines()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Pastikan text adalah string\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    \n",
    "    # Menghilangkan karakter berulang\n",
    "    text = re.sub(r'(.)\\1+', r'\\1', text)\n",
    "    \n",
    "    # Menghilangkan angka\n",
    "    text = ''.join([i for i in text if not i.isdigit()])\n",
    "    \n",
    "    # Menghilangkan tanda baca\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Mengubah teks menjadi huruf kecil\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Melakukan stemming pada teks\n",
    "    text = stemmer.stem(text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "@memory.cache\n",
    "def preprocess_parallel(text_series):\n",
    "    return Parallel(n_jobs=-1)(delayed(preprocess_text)(text) for text in text_series)\n",
    "\n",
    "# Fungsi untuk melakukan preprocessing dan menyimpan hasilnya\n",
    "def process_and_save_data():\n",
    "    print(\"Memulai preprocessing data...\")\n",
    "    \n",
    "    # Membaca file CSV\n",
    "    df = pd.read_csv(data)\n",
    "    \n",
    "    # Cek apakah hasil preprocessing sudah ada\n",
    "    if os.path.exists(preprocessed_file):\n",
    "        print(\"Loading preprocessed texts from cache...\")\n",
    "        with open(preprocessed_file, 'rb') as f:\n",
    "            preprocessed_texts = pickle.load(f)\n",
    "        df['Teks'] = preprocessed_texts\n",
    "    else:\n",
    "        print(\"Preprocessing texts (this might take a while)...\")\n",
    "        import time\n",
    "        start_time = time.time()\n",
    "        preprocessed_texts = preprocess_parallel(df['Teks'].fillna(''))\n",
    "\n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "        print()\n",
    "        print(f\"Waktu mulai: {start_time}\")\n",
    "        print(f\"Waktu selesai: {end_time}\")\n",
    "        print(f\"Waktu eksekusi: {execution_time} detik\")\n",
    "        print()\n",
    "\n",
    "        df['Teks'] = preprocessed_texts\n",
    "        # Simpan hasil preprocessing\n",
    "        with open(preprocessed_file, 'wb') as f:\n",
    "            pickle.dump(preprocessed_texts, f)\n",
    "    \n",
    "    # Simpan dataframe\n",
    "    dump(df, df_file)\n",
    "    \n",
    "    # Cek apakah model TF-IDF dan matrix sudah ada\n",
    "    if os.path.exists(tfidf_vectorizer_file) and os.path.exists(tfidf_matrix_file):\n",
    "        print(\"Loading TF-IDF model and matrix from cache...\")\n",
    "        tfidf = load(tfidf_vectorizer_file)\n",
    "        tfidf_matrix = load(tfidf_matrix_file)\n",
    "    else:\n",
    "        print(\"Creating TF-IDF matrix...\")\n",
    "        # Inisialisasi TfidfVectorizer dengan stop words bahasa Indonesia\n",
    "        tfidf = TfidfVectorizer(stop_words=stop_words_id, max_df=0.85, min_df=2, ngram_range=(1, 2))\n",
    "        # Melakukan fit dan transformasi pada kolom Teks\n",
    "        tfidf_matrix = tfidf.fit_transform(df['Teks'])\n",
    "        # Simpan model dan matrix\n",
    "        dump(tfidf, tfidf_vectorizer_file)\n",
    "        dump(tfidf_matrix, tfidf_matrix_file)\n",
    "    \n",
    "    # Cek apakah model SVD dan matrix yang direduksi sudah ada\n",
    "    if os.path.exists(svd_model_file) and os.path.exists(tfidf_matrix_reduced_file):\n",
    "        print(\"Loading SVD model and reduced matrix from cache...\")\n",
    "        svd = load(svd_model_file)\n",
    "        tfidf_matrix_reduced = load(tfidf_matrix_reduced_file)\n",
    "    else:\n",
    "        print(\"Reducing dimensions with SVD...\")\n",
    "        # Menggunakan TruncatedSVD untuk pengurangan dimensi\n",
    "        svd = TruncatedSVD(n_components=100)\n",
    "        tfidf_matrix_reduced = svd.fit_transform(tfidf_matrix)\n",
    "        # Simpan model dan matrix\n",
    "        dump(svd, svd_model_file)\n",
    "        dump(tfidf_matrix_reduced, tfidf_matrix_reduced_file)\n",
    "    \n",
    "    print(\"Preprocessing selesai dan data telah disimpan!\")\n",
    "    \n",
    "    return df, tfidf, tfidf_matrix, svd, tfidf_matrix_reduced\n",
    "\n",
    "# Fungsi untuk memuat data yang telah diproses\n",
    "def load_processed_data():\n",
    "    if not (os.path.exists(df_file) and \n",
    "            os.path.exists(tfidf_vectorizer_file) and \n",
    "            os.path.exists(tfidf_matrix_file) and \n",
    "            os.path.exists(svd_model_file) and \n",
    "            os.path.exists(tfidf_matrix_reduced_file)):\n",
    "        return process_and_save_data()\n",
    "    \n",
    "    print(\"Loading preprocessed data from cache...\")\n",
    "    df = load(df_file)\n",
    "    tfidf = load(tfidf_vectorizer_file)\n",
    "    tfidf_matrix = load(tfidf_matrix_file)\n",
    "    svd = load(svd_model_file)\n",
    "    tfidf_matrix_reduced = load(tfidf_matrix_reduced_file)\n",
    "    \n",
    "    return df, tfidf, tfidf_matrix, svd, tfidf_matrix_reduced\n",
    "\n",
    "# Path untuk menyimpan feedback relevansi\n",
    "feedback_path = 'relevance_feedback.pkl'\n",
    "\n",
    "# Memuat feedback relevansi jika file ada, jika tidak, buat dictionary kosong\n",
    "try:\n",
    "    with open(feedback_path, 'rb') as f:\n",
    "        relevance_feedback = pickle.load(f)\n",
    "except FileNotFoundError:\n",
    "    relevance_feedback = {}\n",
    "\n",
    "def search_documents(query, tfidf, svd, tfidf_matrix_reduced, df, top_n=10):\n",
    "    # Preprocessing query\n",
    "    query = preprocess_text(query)\n",
    "    \n",
    "    # Transformasi query menjadi vektor tf-idf\n",
    "    query_vec = tfidf.transform([query])\n",
    "    query_vec_reduced = svd.transform(query_vec)\n",
    "    \n",
    "    # Menghitung cosine similarity antara query dan semua dokumen\n",
    "    cosine_similarities = linear_kernel(query_vec_reduced, tfidf_matrix_reduced).flatten()\n",
    "    \n",
    "    # Mendapatkan indeks dokumen dengan similarity tertinggi\n",
    "    related_docs_indices = cosine_similarities.argsort()[-top_n:][::-1]\n",
    "    \n",
    "    # Mendapatkan judul, teks, dan nilai similarity dari dokumen yang relevan\n",
    "    results = [(df.iloc[i]['Judul'], df.iloc[i]['Teks'], cosine_similarities[i], i) for i in related_docs_indices]\n",
    "    \n",
    "    return results\n",
    "\n",
    "def get_feedback(results):\n",
    "    feedback = []\n",
    "    for idx, (title, text, similarity, doc_index) in enumerate(results):\n",
    "        print(f\"Dokumen {idx + 1}:\")\n",
    "        print(f\"Judul: {title}\")\n",
    "        print(f\"Teks: {text[:200]}...\")  # Display only the first 200 characters\n",
    "        print(f\"Similarity: {similarity:.4f}\")\n",
    "        print()\n",
    "        \n",
    "        # Ubah input di sini\n",
    "        while True:\n",
    "            relevansi_input = input(\"Apakah dokumen ini relevan? (1/ya untuk relevan, 0/tidak untuk tidak relevan): \").lower()\n",
    "            if relevansi_input in [\"1\", \"ya\"]:\n",
    "                relevansi = 1  # Relevan\n",
    "                break\n",
    "            elif relevansi_input in [\"0\", \"tidak\"]:\n",
    "                relevansi = 0  # Tidak relevan\n",
    "                break\n",
    "            else:\n",
    "                print(\"⚠︎⚠︎⚠︎ Input tidak valid. Masukkan 1/ya atau 0/tidak. ⚠︎⚠︎⚠︎\")\n",
    "        \n",
    "        feedback.append((doc_index, relevansi))\n",
    "    return feedback\n",
    "\n",
    "def optimize_with_feedback(feedback, tfidf_matrix_reduced):\n",
    "    # Ubah logika penentuan dokumen relevan/tidak relevan\n",
    "    relevant_docs = [idx for idx, relevansi in feedback if relevansi == 1]  # Jika input 1 atau \"ya\"\n",
    "    non_relevant_docs = [idx for idx, relevansi in feedback if relevansi == 0]  # Jika input 0 atau \"tidak\"\n",
    "    \n",
    "    if not relevant_docs:\n",
    "        print(\"Tidak ada dokumen yang dianggap relevan. Pencarian ulang tidak dapat dilakukan.\")\n",
    "        return None\n",
    "    \n",
    "    relevant_matrix = tfidf_matrix_reduced[relevant_docs]\n",
    "    non_relevant_matrix = tfidf_matrix_reduced[non_relevant_docs] if non_relevant_docs else np.zeros(relevant_matrix.shape)\n",
    "    \n",
    "    # Compute the centroid of relevant and non-relevant documents\n",
    "    relevant_centroid = np.asarray(relevant_matrix.mean(axis=0)).flatten()\n",
    "    non_relevant_centroid = np.asarray(non_relevant_matrix.mean(axis=0)).flatten() if non_relevant_docs else np.zeros(relevant_centroid.shape)\n",
    "    \n",
    "    # Update query vector by moving it towards the relevant centroid and away from the non-relevant centroid\n",
    "    def adjust_query_vec(query_vec, relevant_centroid, non_relevant_centroid, alpha=1, beta=0.75, gamma=0.15):\n",
    "        return alpha * query_vec + beta * relevant_centroid - gamma * non_relevant_centroid\n",
    "    \n",
    "    return adjust_query_vec\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Load preprocessed data\n",
    "    df, tfidf, tfidf_matrix, svd, tfidf_matrix_reduced = load_processed_data()\n",
    "    \n",
    "    query = input(\"Masukkan Query: \")\n",
    "    print(f\"Query: {query}\\n\")\n",
    "    print()\n",
    "    \n",
    "    if query in relevance_feedback:\n",
    "        print(\"Menggunakan feedback relevansi yang telah disimpan...\\n\")\n",
    "        feedback = relevance_feedback[query]\n",
    "        adjust_query_vec = optimize_with_feedback(feedback, tfidf_matrix_reduced)\n",
    "        \n",
    "        if adjust_query_vec:\n",
    "            relevant_docs = [idx for idx, relevansi in feedback if relevansi == 1]\n",
    "            non_relevant_docs = [idx for idx, relevansi in feedback if relevansi == 0]\n",
    "            \n",
    "            # Reprocess the query with the adjusted query vector\n",
    "            query_vec = tfidf.transform([preprocess_text(query)])\n",
    "            query_vec_reduced = svd.transform(query_vec)\n",
    "            adjusted_query_vec = adjust_query_vec(query_vec_reduced, np.asarray(tfidf_matrix_reduced[relevant_docs].mean(axis=0)).flatten(), \n",
    "                                                  np.asarray(tfidf_matrix_reduced[non_relevant_docs].mean(axis=0)).flatten() if non_relevant_docs else np.zeros(query_vec_reduced.shape))\n",
    "            \n",
    "            # Compute cosine similarity with the adjusted query vector\n",
    "            cosine_similarities = linear_kernel(adjusted_query_vec, tfidf_matrix_reduced).flatten()\n",
    "            related_docs_indices = cosine_similarities.argsort()[-10:][::-1]\n",
    "            \n",
    "            # Display optimized results\n",
    "            optimized_results = [(df.iloc[i]['Judul'], df.iloc[i]['Teks'], cosine_similarities[i]) for i in related_docs_indices]\n",
    "            print(\"\\n\\n -- HASIL PENELUSURAN ULANG -- \\n\\n\")\n",
    "            for idx, (title, text, similarity) in enumerate(optimized_results):\n",
    "                print(f\"Dokumen {idx + 1}:\")\n",
    "                print(f\"Judul: {title}\")\n",
    "                print(f\"Similarity: {similarity:.4f}\")\n",
    "                print()\n",
    "    else:\n",
    "        initial_results = search_documents(query, tfidf, svd, tfidf_matrix_reduced, df)\n",
    "        feedback = get_feedback(initial_results)\n",
    "        relevance_feedback[query] = feedback  # Simpan feedback untuk query ini\n",
    "        with open(feedback_path, 'wb') as f:\n",
    "            pickle.dump(relevance_feedback, f)  # Simpan feedback relevansi ke file\n",
    "        adjust_query_vec = optimize_with_feedback(feedback, tfidf_matrix_reduced)\n",
    "        \n",
    "        if adjust_query_vec:\n",
    "            relevant_docs = [idx for idx, relevansi in feedback if relevansi == 1]\n",
    "            non_relevant_docs = [idx for idx, relevansi in feedback if relevansi == 0]\n",
    "            \n",
    "            # Reprocess the query with the adjusted query vector\n",
    "            query_vec = tfidf.transform([preprocess_text(query)])\n",
    "            query_vec_reduced = svd.transform(query_vec)\n",
    "            adjusted_query_vec = adjust_query_vec(query_vec_reduced, np.asarray(tfidf_matrix_reduced[relevant_docs].mean(axis=0)).flatten(), \n",
    "                                                  np.asarray(tfidf_matrix_reduced[non_relevant_docs].mean(axis=0)).flatten() if non_relevant_docs else np.zeros(query_vec_reduced.shape))\n",
    "            \n",
    "            # Compute cosine similarity with the adjusted query vector\n",
    "            cosine_similarities = linear_kernel(adjusted_query_vec, tfidf_matrix_reduced).flatten()\n",
    "            related_docs_indices = cosine_similarities.argsort()[-10:][::-1]\n",
    "            \n",
    "            # Display optimized results\n",
    "            optimized_results = [(df.iloc[i]['Judul'], df.iloc[i]['Teks'], cosine_similarities[i]) for i in related_docs_indices]\n",
    "            print(\"\\n\\n -- HASIL PENELUSURAN ULANG -- \\n\\n\")\n",
    "            for idx, (title, text, similarity) in enumerate(optimized_results):\n",
    "                print(f\"Dokumen {idx + 1}:\")\n",
    "                print(f\"Judul: {title}\")\n",
    "                print(f\"Similarity: {similarity:.4f}\")\n",
    "                print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
